# Model Trainer
We apply transfer learning from the sentence-transformer model in Hugging Face called 'sentence-t5-base' due to its optimal accuracy and complexity, as observed from the model leaderboard in the sentence similarity task. We then utilize this model as a cross-encoder model.
- [STS train dataset](https://docs.google.com/spreadsheets/d/12Y-3dwI43jTe-teXGF3cfKxz1QNefXHQj3v3NJhJBho/edit?usp=sharing). Link to our dataset
- [HuggingFace STS Model Ranking](https://huggingface.co/spaces/mteb/leaderboard). Link to the similarity task model leaderboard.
- [Difference of Bi-Encoder & Cross-Encoder](https://sbert.net/examples/applications/cross-encoder/README.html). Detailed explaination of bi-encoder and cross-encoder
- [STS-trained-lokergo Model](https://huggingface.co/pahri/sts-trained-lokergo). Our trained model was hosted into HuggingFace repository for easy access

C23-VR01 ML Teams.
